 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Das Poster illustriert anhand einer Edition historischer Patiententexte, wie Editionsprojekte mit konsequenter Nutzung etablierter Tools und vorhandener technischer Infrastrukturen auch ohne eigenen Technologiestack über Institutionen verteilt arbeiten und ihre Ergebnisse nachnutzbar präsentieren können. Als Datengrundlage dient das in der Erlanger Nachwuchsforschungsgruppe "Flexible Schreiber in der Sprachgeschichte" entstandene Korpus historischer Patiententexte (CoPaDocs; ca. 1850–1940). Dieses ist im XML/TEI-Format aufbereitet und umfasst über 4.500 Texte aus unterschiedlichen Regionen Deutschlands und Großbritanniens – überwiegend nicht abgeschickte Briefe von Patientinnen und Patienten, aber auch Lebensläufe, Notizen, sowie Briefe ihrer Angehörigen. Die Edition greift auf etablierte, zuverlässige und in der Community breit adaptierte Plattformen wie GitHub zurück, wo die CoPaDocs-Daten zugänglich sind. Für die dynamische Komponente der Volltextsuche über die Editionsinhalte werden die offenen Schnittstellen der Korpusinfrastruktur des Digitalen Wörterbuchs der deutschen Sprache (DWDS) in Verbindung mit der Software zur Normalisierung historischer Schreibweisen des Deutschen Textarchivs (DTA) genutzt.</p><p>Wer auf der Suche nach wissenschaftlicher Online-Kommunikation, Vernetzung und Austausch ist, muss sich dafür nicht mehr in die Hände der mächtigen kommerziellen Social-Media-Plattformen begeben. Das Fediverse stellt dafür eine dezentrale und auf freier Software und offenen Schnittstellen beruhende Alternative dar. Doch wie sollten die ersten (und zweiten) Schritte im Fediverse aussehen? Wo findet man Digital Humanists und Digital Humanities-Themen? Gibt es Tools für das automatisierte Posten à la autoChirp auch für Mastodon? Der halbtägige Workshop widmet sich diesen Fragen aus praktischer Sicht. Der Fokus liegt auf Erkundung, Erprobung und Diskussion von Potenzialen des Microblogging-Dienstes Mastodon für DH-Forschende für Wissenschaftskommunikation, Community-Building und Lehre. Neben der DHd-Mastodon-Instanz Fedihum wird der neue Service für das automatisierte Posten von Tröts autodone vorgestellt und in Praxisübungen Schritt für Schritt zum Einsatz gebracht.</p><p>Prinz Eugen von Savoyen baute neben seiner militärischen Tätigkeit am Habsburgerhof eine der bedeutendsten barocken Bibliotheken auf, die mit rund 15.000 Objekten bis heute einen historischen Kernbestand der Österreichischen Nationalbibliothek bildet. Für die Visualisierung und Vermittlung dieser Sammlung ist — im Kontrast zu vielen Digitalisierungsprojekten — auch die Repräsentation ihrer materiellen Anordnung im Prunksaal der Nationalbibliothek relevant. Neben der Ermöglichung eines hybriden (räumlichen und diagrammatischen) Blicks ergibt sich eine Herausforderung für die Sammlungsvisualisierung auch aus der parallelen Präsenz von multiplen Wissensordnungen. Neben der Klassifikation aller Bücher durch einen historischen Bestandskatalog steuert auch die farblich-typologische Klassifikation durch historische Supralibros-Einbände, sowie die Taxonomie eines modernen Bibliothekskatalog relevante Ordnungsperspektiven zur aktuellen Entwicklung von Distant-Reading Perspektiven bei. Den Fokus der Einreichung bildet vor diesem Hintergrund die Diskussion von Strategien zur multi-klassifikatorischen Sammlungsvisualisierung, welche auch als Herausforderung künftiger DH-Projekte im Feld komplexer historischer Kulturdatenbestände diskutiert wird.</p><p>Die kollaborativ erstellte Online-Enzyklopädie Wikipedia bietet mit derzeit über 60 Millionen Artikeln in über 300 Sprachversionen Informationen zu den unterschiedlichsten Wissensbereichen. Auch die rezeptionsorientierte Literaturwissenschaft hat das <span style="color: rgb(3, 252, 0);">Projekt</span> inzwischen als Forschungsgegenstand und Datenressource entdeckt, da es viele enzyklopädische Beiträge und Metadaten zur Literatur und zum literarischen Leben versammelt, zu Autor*innen, literarischen Werken, Genres, Epochen und anderen literaturgeschichtlich relevanten Kategorien. Die datenanalytische Auswertung verschiedener Wikipedia-Metriken ermöglicht es, die Auseinandersetzung mit Literatur in Wikipedia evaluierbar zu machen und Aussagen über literarische Kanonizität, Wertungspraktiken und Popularität im Kontext offener Enzyklopädieprojekte weiter zu diversifizieren. Im Zentrum des (hands-on) Workshops steht die Wikipedia-API, mit deren Funktionsweise die Teilnehmer*innen vertraut gemacht werden. Sukzessive werden Abfrageskripte in Form eines Jupyter Notebooks erarbeitet.</p><p>This contribution describes the curation and analysis of "XVIIIe: Bibliographie", published by Benoît Melançon in regular installments since 1992. The bibliography is focused on scholarly publications about the (French) Eighteenth century. In 2023, Melançon published a complete set of references for the years 1992-2022, containing about 64.400 entries. The tabular format provided by Melançon was first transformed into BibTeX for publication on Zotero, where the bibliography can now be consulted. The <span style="color: rgb(2, 253, 0);">data</span> was also exported to Zotero RDF for analysis. So far, analyses have been performed concerning the publication types, languages, themes and co-authorship patterns that can be observed in the bibliography. The poster aims to showcase this unique resource, provide insight into strategies for the curation and modeling of bibliographic data, and highlight some of the findings about the publication habits of the research community whose work is documented by the bibliography. Further information:</p><p>Deep Learning hat bereits neue Erkenntnisse in den digitalen Geisteswissenschaften ermöglicht. Vormoderne Sprachen und Sprachen des globalen Südens bringen allerdings Herausforderungen mit sich, die aktuell diesen analytischen Zugriff in diesem Bereich noch nicht erlauben. Das <span style="color: rgb(3, 252, 0);">Projekt</span> "The Flow" entwickelt Lösungen für solche historische Korpora in den Bereichen Handschriftenerkennung, Entitätsidentifikation, Event-Extraktion, Topic Modeling und Clustering. Die Entwicklung der Webanwendung nopaque zielt <span style="color: rgb(3, 252, 0);">darauf</span> ab, diese bestehenden Methoden bzw. Werkzeuge in einem übergreifenden Workflow zu verbinden. Der Workshop stellt den aktuellen Stand von nopaque bzw. den Workflow vor. Ziel ist es, bei der Etablierung eines allgemein anwendbaren Workflows für Deep Learning die Vielfalt der Quellen und geisteswissenschaftlichen Forschung zu berücksichtigen. Wir laden Teilnehmer:innen ein, Ideen und Erfahrungen einzubringen und Implementierungen für nicht standardisierte Layouts, Schriften und Sprachen zu diskutieren. Der Workshop trägt dazu bei, unser Projektziel zu erreichen: maschinelles Lernen in allen Bereichen der Geschichtswissenschaft zugänglicher zu machen.</p><p>Diese Studie untersucht den Einsatz von Large Language Models (LLMs) wie GPT-4 im Kontext der Digital Humanities, am Beispiel des Overhauling und Refactoring des Projekts "Urfehdebücher der Stadt Basel" (UFBAS). GPT-4 unterstützt verschiedene Schritte des Prozesses, darunter Datenmodellierung, -transformation und -visualisierung sowie Softwareentwicklung und Datenvisualisierung. Es werden TEI XML, Kategorien und SKOS, Ontologie, RDF, Webentwicklung und Datenvisualisierungen optimiert, um die Usability, Effizienz und Datenqualität zu verbessern. Die Ergebnisse und Erfahrungen dieses Overhauls werden dokumentiert und sollen dazu beitragen, das Potenzial von LLMs und generativer KI für digitale Editionsprojekte in den Geisteswissenschaften weiter auszuloten.</p><p>Der Workshop zielt im Sinne der FAIR-Prinzipien und im Hinblick auf konkrete Standardisierungsbestrebungen auf die Vernetzung geistes- und kulturwissenschaftlicher Daten durch Erstellung sogenannter BEACON-Dateien auf Basis der Gemeinsamen Normdatei (GND). In dem Workshop werden zunächst zwei Beispiele vorgeführt, an denen das grundlegende Vorgehen erläutert wird. Der Schwerpunkt liegt allerdings auf der praktischen Anwendung und Hands-On-Erprobung des BEACON-Verfahrens. Abgerundet wird die Veranstaltung durch Ausblicke auf erweiterte Anwendungsbereiche des Formats.</p><p>RaDiHum20 (Radio Digital Humanities 20) ist ein ehrenamtliches Podcastprojekt für die science-to-science-Kommunikation in den Digital Humanities. Seit 2020 ist der Podcast Teil der DH-Community und hat in vielen Gesprächen mit Forschenden sowie Mitgliedern, Convenor*innen und Vorständen des DHd-Verbandes die DH-Community begleitet. Dadurch kann RaDiHum20 (wie auch andere, vergleichbare Podcasts) nicht nur als Wissenschaftskommunikations-Projekt, sondern auch als eine Quelle gegenwartsorientierter Wissenschaftsforschung der DH (im Sinne der Oral History) verstanden werden. Das Poster stellt daher nicht nur den Podcast RaDiHum20 und seine redaktionellen Workflows vor, sondern stärkt auch die These, dass Podcasting als Indikator für Trends in der Wissenschaft (in diesem Fall: den DH) herangezogen werden kann. RaDiHum20 ist damit auch ein (radioaktiver) Spiegel der Fachcommunity.</p> </html>