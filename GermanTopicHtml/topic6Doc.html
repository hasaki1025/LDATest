 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Klösterliche Rechnungsbücher erlauben sehr vielfältige Einblicke, in den Alltag der Mönche ebenso wie in die sogenannte "große" Geschichte. Und nicht zuletzt werden in diesen Quellen natürlich auch die wirtschaftlichen Angelegenheiten eines Klosters umfänglich dokumentiert. Die Aussagekraft und der Quellenwert dieser Texte sind somit sehr hoch anzusetzen. Rechnungsbücher erlauben vielfach Einblicke, die sonst verwehrt blieben. Oft wurden nämlich - unabsichtlich und daher zumeist umso wertvoller - Ereignisse und Vorkommnisse dokumentiert, die sonst in der übrigen schriftlichen Überlieferung fehlen. Das Aldersbach in der Nähe von Passau kann dabei als idealtypisches Beispiel für ein bayerisches Zisterzienserkloster während des 15. und 16. Jahrhunderts gelten. Zwar ist die Geschichte dieses Klosters selbst vergleichsweise schlecht erforscht, die gute Überlieferung gerade der Rechnungen im 15. Jahrhundert selbst sowie die Tatsache, dass Aldersbach eine vergleichsweise durchschnittliche religiöse Gemeinschaft war, die es in vergleichbarer Größe hundertfach in ganz Europa gab, lassen das niederbayerische Zisterzienserkloster als geradezu prototypisch erscheinen.</p><p>Die Möglichkeiten, die verschiedenen Programme im Bereich automatisierter Texterkennung heutzutage bieten, sind vielfältig. Deren Anwendung sowie die Vor- und Nachverarbeitung der Digitalisate ist jedoch nicht immer intuitiv. Im Projekt OCR-BW haben die Universitätsbibliotheken Mannheim und Tübingen seit 2019 das "Kompetenzzentrum Volltexterkennung von handschriftlichen und gedruckten Werken" aufgebaut und beraten seitdem Informationseinrichtungen und wissenschaftliche Projekte in Baden-Württemberg zu diesem Thema. Das umfangreiche Know-how im Bereich automatisierte Texterkennung und die verschiedenen Serviceangebote des Kompetenzzentrums sollen hier erläutert werden und Wissenschaftler*innen hinsichtlich der Einsatzmöglichkeiten von Texterkennungssoftware informiert werden.</p><p>Das Paper schlägt mit ATAG ausgehend von TagML eine explizite Modellierung von Text in einer Labeled-Property-Graphdatenbank vor. Das Grundmuster ist eine Kette von adressierbaren Zeichenknoten mit einer Ebene von Annotationsknoten. Alles weitere setzt sich dann aus weiteren dieser Grundmuster zusammen. Das Modell wird schon produktiv in den Publikationsumgebungen der DFG-Projekte sozinianer.de und liberepistorlarum.de eingesetzt. Aktuell wird an einem webbasierten Editor gearbeitet, mit dem die Eingabe und die Änderung von bereits vorhandenen Text ermöglicht werden soll. Damit bietet das Modell die Robustheit der in der Computerlinguistik schon lange verwendeten Standoff-Annotationsformate verbunden mit der Möglichkeit, bereits annotierten Text zu ändern.</p><p>Im <span style="color: rgb(1, 254, 0);">Rahmen</span> von Henze-Digital (digitale Briefausgabe) steht die Entwicklung von Methoden zur Nachnutzung bestehender Infrastruktur im Vordergrund. Im Wesentlichen handelt es sich hier um die beiden Teilbereiche der Datenschemata und der Forschungssoftware. Auf der einen Seite stehen die Forschungsdatenstrukturen, die in diesem Fall auf TEI-ODDs aufbauen. Nachnutzungsmöglichkeiten für diese Schemata bietet bereits der TEI-Standard mit ODD-Chaining. Komplexer gestaltete sich jedoch die Weiterentwicklung der Forschungssoftware WeGA-WebApp, die nicht separat, sondern in Abhängigkeit des Originals vollzogen werden sollte, sodass auch die Rückführung von Erweiterungen möglich ist. Der Vortrag soll einen Überblick über die entwickelte Methodik, Ansätze aber auch Fehlschläge geben, die an ausgewählter Beispiele erörtert werden.</p><p>Die visuelle Repräsentation von literarischen Phänomenen ist ein etablierter Ansatz in den Computational Literary Studies, um die aus Texten extrahierten Daten bzw. abgeleiteten Strukturen zu explorieren und zu interpretieren. Vor diesem Hintergrund soll das vorgeschlagene Poster die Ergebnisse einer Rezeptionsstudie präsentieren, mit der die heuristische Qualität von den im Projekt EvENT ("Evaluating Events in Narrative Theory") bislang generierten Narrativitätsgraphen überprüft wurde. Ziel der Studie war es daher, die Erkennbarkeit der den Graphen zugrunde liegenden Texte zu untersuchen, um hieraus Rückschlüsse für die Weiterentwicklung des EvENT-Ansatzes wie auch die Anwendbarkeit der Graphen für die literaturwissenschaftlich-hermeneutische Praxis zu ziehen.</p><p>Ziel der Diskussion ist es, verschiedene Fragestellungen und Herangehensweisen zu erörtern, um unterschiedliche Zielgruppen in Digital Humanities (DH) und Cultural Heritage (CH) zu erreichen und in einen Diskurs miteinzubeziehen, sowie auftretende Schwierigkeiten und Probleme in diesem Prozess aufzugreifen. Die Teilnehmer:innen des Panels setzen sich aus vier in Deutschland, Österreich, der Schweiz, den Niederlanden und Estland agierenden Vorhaben und Projekten zusammen, die sich die Aufbereitung, Bereitstellung und Benutzung von CH im Kontext der DH für verschiedene Gruppen zum Ziel gesetzt haben.</p><p>In den digitalen Geschichtswissenschaften werden Geodaten und historischen Ortsdaten bisher zumeist alleinig unter Verwendung von Geoinformationssystemen (GIS), oftmals v.a. zur Visualisierung der Daten, bearbeitet. Im <span style="color: rgb(1, 254, 0);">Rahmen</span> des Projekts HisMaComp wird die historische und räumlich-topographische Entwicklung von Städten mittels eines datengetriebenen Ansatzes komparativ analysiert. Dafür wurden sechs deutsch-polnische case studies ausgewählt. Die Forschungsfrage dahinter lautet: Wie lassen sich Topographie, Funktionalität und Morphologie historischer Stadträume erfassen und vergleichend analysieren? Für die gemeinsame Analyse all dieser Aspekte sind GIS allein nicht ausreichend. Durch die Kombination von GIS und Semantic Web-Technologien lässt sich die Komplexität der zu verarbeitenden Informationen jedoch erhöhen, sodass tiefergehende und multidimensionale komparatistische Analysen möglich werden. Diese Kombination von mehreren Methoden bedarf einer methodenkritischen Auseinandersetzung. Das geplante Poster zeigt anhand eines illustrativen Fallbeispiels den Prozess von der Forschungsfrage über die Anforderungsanalyse hin zum ontology engineering.</p><p>Der Vortrag präsentiert die Ergebnisse eines Projekts, das sich die Erschließung von Diskettenmagazinen der 1980er und 1980er Jahre zum Ziel gesetzt hat. Die sogenannten "Diskmags", verbreitet ausschließlich auf Floppy-Disks, stellten in der Zeit vor dem Web 2.0 ein bedeutendes "born digital"-Medium dar, in dem sich unterschiedlichste frühe digitale Kulturszenen durch Text, Bild, Animation und Ton untereinander austauschten. In dem vorgestellten Projekt entstand ein Katalog, in dem mehrere tausend Titel und weit über zehntausend Ausgaben erstmals wissenschaftlich und systemübergreifend recherchierbar sind. Mithilfe von Textmining-Methoden wurden aus den Binärdateien deutschsprachiger Magazine ein Textkorpus erstellt, das nicht nur für die Volltextsuche, sondern auch für die Erforschung der Sprache und der Thematiken der frühen digitalen Kultur nachgenutzt werden kann. Der Vortrag wertet die Projektergebnisse aus und stellt die angewendeten Methoden zur Diskussion.</p><p>Digitale Editionen sollten es eigentlich möglich machen, dass auch Werke, Autoren und Autorinnen ausserhalb des Kanons sichtbar gemacht werden. In der Praxis ist aber auch im digitalen Editionswesen der Fokus immer noch relativ stark auf den Kanon. Dies ist zum Teil dem Faktum geschuldet, dass es wenig niedrigschwellige Werkzeuge zur Erzeugung digitaler Editionen gibt. Das μEdition Projekt hat das Ziel die Schwellen für die Erstellung digitaler Editionen zu reduzieren. Dazu hat das Projekt einen Softwarestack entwickelt, der es Teams jeder Größe und ohne Finanzierung ermöglicht, schnell und einfach digitale Editionen zu entwickeln, zu bearbeiten, und zu veröffentlichen. Das Projekt hofft mit diesem Beitrag die Editionslandschaft breiter und offener zu machen.</p> </html>