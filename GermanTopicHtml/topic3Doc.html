 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Das Projekt »textklang« der Universität Stuttgart und des DLA Marbach untersucht die Beziehung zwischen literarischen Texten, insbesondere Lyrik, und ihrer lautsprachlichen Realisierung. Dabei werden computerlinguistische Ansätze, digitale phonetische und sprachtechnologische Verfahren, sowie Explorations- und Visualisierungstechniken mit hermeneutischen Methoden der literaturwissenschaftlichen Textanalyse kombiniert. Das im Projekt erstellte Audio-Korpus umfasst derzeit 1744 Rezitationen von gut 1000 Gedichten hauptsächlich aus der Zeit der Romantik. Metadaten, Texte und Audioaufnahmen werden in einem Online-Repositorium abgelegt und Text- und Audioebene umfangreich automatisch annotiert. Zwei Hauptwerkzeuge werden vorgestellt: das MD-Dashboard zur Exploration der Metadaten und der Textklang-Explorer, der komplexe Suchanfragen an das Korpus auf verschiedenen Annotationsebenen ermöglicht. Die vorgestellten Ressourcen und Tools eröffnen neue Forschungsfragen und bieten eine vielfältige Exploration der multimodalen Daten, indem sie Informationen auf verschiedenen Ebenen zusammenführen. In unserem Poster stellen wir die genannten Tools vor und diskutieren anhand von Anwendungsfällen die Herausforderungen und Chancen der integrativen Analyse von Text und Ton.</p><p>Das forTEXT-Projekt ist ein Disseminationsprojekt, das seit 2017 die Verbreitung digitaler Methoden in Literaturwissenschaften vorantreibt. Zentraler Bestandteil des Projekts ist das Portal forTEXT.net. In einer weiteren Projektphase wird das Portal durch die Einführung des Open-Access-Journals forTEXT-Hefte erweitert werden. In den forTEXT-Heften werden die Portalinhalte als thematisch gebündelte Ausgaben publiziert. Forschende und Lehrende erhalten außerdem die Möglichkeit, die Themenhefte durch eigene Beiträge zu erweitern. Dadurch entsteht eine dynamische Plattform für den Wissensaustausch, auf der Erfahrungen, Erkenntnisse und bewährte Methoden geteilt werden können. Die forTEXT-Hefte werden als verlagsunabhängiges Journal betrieben und tragen dazu bei, Wissen und Methoden in den digitalen Literaturwissenschaften nachhaltig zugänglich zu machen. Das Poster gibt einen Einblick in die forTEXT-Hefte und stellt den technisch-redaktionellen Workflow des Journals dar.</p><p>Der Vortrag beschäftigt sich mit der Nutzung von Glockengussdaten als Proxy-Variable, um Einblicke in die wirtschaftliche Entwicklung in Deutschland zu gewinnen. Historische Daten über Glockengüsse vor 1940 werden als Indikator für das Wirtschaftswachstum verwendet, da Glocken eine kostspielige und dauerhafte Investition waren. Die Digitalisierung und anschließende Transkription und Visualisierung der Glockengussdaten mit Hilfe von OCR-Software QGIS werden vorgeführn. ZUsätzlich ist die Erfassung der Karteikarten der für die Einschmelzung bestimmten B- und C-Glocken geplant.</p><p>Dieser Artikel versucht sich an einem Mapping grundlegender DH-Teilbereiche auf das Forschungsfeld der Game Studies. Dabei fällt auf, dass insbesondere der Teilbereich der "Computational Humanities" in den Game Studies bislang stark unterrepräsentiert ist. Der Beitrag schlägt drei Annäherungsperspektiven für "Computational Game Studies" vor, um so einen Dialog in der DH-Community zu starten und künftig weitere Studien in diesem Bereich zu befördern.</p><p>Abraham Gottlob Werner (1749–1817) widmete mehr als 40 Jahre (1775 bis 1817) seiner Lehrtätigkeit und <span style="color: rgb(2, 253, 0);">Forschung</span> an der Bergakademie Freiberg. Seine wissenschaftlichen Thesen, Erkenntnisse und Publikationen sowie sein Engagement als Lehrer und sein Einfluss auf eine Vielzahl von Schülern prägten maßgeblich die Entwicklung der Geowissenschaften, insbesondere in den Bereichen Mineralogie und Geologie. Die von ihm erstellten Systematiken und Sammlungen werden in unserer Arbeit in einen strukturierten Datensatz überführt, sodass dieser für wissenschaftshistorische Auswertungen und Analysen genutzt werden kann. Der Wert von historischen wissenschaftlichen Sammlungen wird oft hinterfragt, insbesondere wenn es sich um Sammlungen handelt, die in ihrem ursprünglichen Fachgebiet keinen direk-ten Nutzen mehr für <span style="color: rgb(2, 253, 0);">Forschung</span> oder Lehre haben. Sie waren jedoch oft von großer <span style="color: rgb(5, 250, 0);">Bedeutung</span> für die Entstehung und Entwicklung von wissenschaftlichen Disziplinen und sind daher eine wichtige Primärquelle für wissenschaftshistorische Forschung.</p><p>The workshop invites scholars in the arts and humanities, irrespective of their background, to explore the innovative platform of OpenMethods metablog. The nominated papers for this platform are assessed based on seven criteria: Scope, Openness, Relevance, Clarity, Diversity, Language, and Assessment and Validation. During the workshop, these criteria will be thoroughly discussed, such as whether nominated contents should only be about non-peer-reviewed formats like blog posts or podcasts. The workshop aims to engage participants in conceptual discussions, evaluating nominations, and crafting introductions for the metablog. The ultimate goal is to foster sustainability through community involvement and encourage participants to spread the word and collaborate with the platform.</p><p>Die Möglichkeiten, die verschiedenen Programme im Bereich automatisierter Texterkennung heutzutage bieten, sind vielfältig. Deren Anwendung sowie die Vor- und Nachverarbeitung der Digitalisate ist jedoch nicht immer intuitiv. Im Projekt OCR-BW haben die Universitätsbibliotheken Mannheim und Tübingen seit 2019 das "Kompetenzzentrum Volltexterkennung von handschriftlichen und gedruckten Werken" aufgebaut und beraten seitdem Informationseinrichtungen und wissenschaftliche Projekte in Baden-Württemberg zu diesem Thema. Das umfangreiche Know-how im Bereich automatisierte Texterkennung und die verschiedenen Serviceangebote des Kompetenzzentrums sollen hier erläutert werden und Wissenschaftler*innen hinsichtlich der Einsatzmöglichkeiten von Texterkennungssoftware informiert werden.</p><p>In der Präsentation werden die Resultate des Projekts "Mediatheken der Darstellenden Kunst digital vernetzen" vorgestellt. Entwickelt wurde eine Webplattform, basierend auf Linked Data-Technologien, die Metadaten zu audiovisuellen Aufzeichnungen von Tanz- und Theateraufführungen zusammenführt. Die Daten aus institutionellen Mediatheken der Theaterwissenschaft werden von Projektpartner:innen zur Verfügung gestellt. Im Zuge dessen wurde eine Ontologie erstellt, auf der aufbauend eine digitale Infrastruktur <span style="color: rgb(2, 253, 0);">erstellt</span> wurde, die eine Ingest-Pipeline, eine Triplestore-Datenbank sowie ein Backend und ein Frontend umfasst. Für Forscher:innen besonders der Tanz- und Theaterwissenschaft ergeben sich damit neue Suchmöglichkeiten um Material zu Aufführungen zu finden. Des weiteren bietet die Datensammlung interessante Ergebnisse und Potentiale für digitale Verfahren, die im Vortrag vorgestellt werden. Aufgezeigt werden zudem die mit dem Projekt verbundenen Herausforderungen - z.B. rechtliche Fragestellungen - und wie solche Plattformen zur Gedächtnispflege der Darstellenden Künste beitragen können.</p><p>Das Interesse der DH-Community an Film- und Videoanalysen hat in den letzten Jahren stark zugenommen. Videos sind typischerweise multimodal und Analysen auf Basis manueller Annotationen nehmen viel Zeit in Anspruch. Deshalb sind Informatik-Methoden der Mustererkennung zur (semi-)automatischen Auswertung der verschiedenen Modalitäten von hoher Bedeutung, da sie Forscher:innen aus den Digital Humanities Informationen über die interne Dynamik einzelner Videos oder ganzer Korpora liefern können. Allerdings schreiten die Entwicklungen im Bereich der Künstlichen Intelligenz und dem Teilgebiet des maschinellen Lernens schnell voran und die Implementierung aktueller Methoden erfordert umfassendes technisches Verständnis und viele Rechenressourcen. Abhilfe schafft die webbasierte Plattform "TIB AV-Analytics" (TIB-AV-A), die einen niederschwelligen Zugang zur systematischen Film- und Videoanalyse anbietet. Ziel des Workshops ist, in TIB-AV-A einzuführen, Hands-on-Erfahrung zu vermitteln, Datenimport und -export vorzustellen und darüber hinaus aktiv an einer Community für TIB-AV-A zu arbeiten. Deshalb kooperieren die Entwickler:innen von TIB-AV-A auch mit der DHd AG Film & Video.</p> </html>