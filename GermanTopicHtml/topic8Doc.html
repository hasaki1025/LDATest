 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Mit dem 2023 begonnenen und auf 13 Jahre angesetzten Akademie-Projekt "Heinrich Scholz und die Schule von Münster – Mathematische Logik und Grundlagenforschung" gilt es nicht nur, den Nachlass eines bedeutenden und hochgradig vernetzten Mathematikers und Logikers zu erschließen und zu editieren, sondern ebenfalls ein umfangreiches semantisches Netzwerk zu erstellen. Katalogisierung, Digitalisierung und automatische Texterkennung des an in der ULB Münster liegenden Nachlasses Scholz sind die ersten Bausteine der Erschließung, die sich nur dann erfolgreich ergänzen und Folgeschritte sinnvoll vorbereiten, wenn die Schnittstellen zwischen Menschen und Maschinen ausgehandelt sind. Vorhandene Infrastrukturen, der Zustand des Nachlasses und ein interdisziplinäres Projektteam sind Rahmenbedingungen, die kommunikativ und technisch in einem gemeinsamen Workflow verankert werden müssen.</p><p>Das historische Grundbuch der Stadt Basel ist eine handschrifltich erstelltes Korpus mit mehr als 100000 Einträgen zu wirtschaftlichen Transaktionen in der Stadt Basel zwischen 1300 und 1800. Der digitalisierte Zettelkaste eröffnet denn auch Einblicke in soziale, wirtschaftliche und kulturelle Praktiken einer sich wandelnden Stadt. Mittels Ansätzen der Layoutanalyse, Texterkennung und Informationsextraktion werden die unterschiedlichen Datenstände extrahiert und zugänglich gemacht. Dabei müssen maschinelle Lernverfahren eingesetzt und ihre Leistungsfähigkeit kritisch evaluiert werden. Als Resultat wird eine zeitliche und räumliche Analyse automatisch annotierter und ausgewerteter Befunde zu Liegenschaften und deren Bewohner*innen möglich, die Chancen und Risiken des Einsatzes neuronaler Netze aufzeigen.</p><p>Im ganztägigen Workshop erlangen die Teilnehmenden erforderliche Kenntnisse, um <span style="color: rgb(5, 250, 0);">Tools</span> und Workflows für die Volltexterschließung unter der Vielzahl von Angeboten auszuwählen. Dabei legen wir einen besonderen Fokus auf Open-Source-Produkte wie OCR-D und OCR4all. Nach einer Einführung in OCR-Technologien folgen praktische Teile, in denen die jeweiligen <span style="color: rgb(5, 250, 0);">Tools</span> für je ca. 1,5 Stunden demonstriert und ausprobiert werden. Ergänzt werden diese um kurze Inputvorträge zu praktischen Fragen wie der Erstellung eines geeigneten Workflows oder zu Hinweisen, was es zu beachten gilt bei Förderanträgen für OCR-Projekte.</p><p>Im Wissenschaftspodcast #arthistoCast dreht sich alles um die Digitale Kunstgeschichte. Dabei geht es um den Einsatz digitaler Methoden in der kunsthistorischen Forschung, also um die Frage, wie technische Entwicklungen für das Fach genutzt werden können und wie sich die Forschung im Zuge der Digitalisierung verändert hat.</p><p>Mit der Veröffentlichung von ChatGPT durch das US-amerikanische Unternehmen OpanAi im vergangenen Jahr erhielt der durch die Coronakrise ausgelöste Digitalisierungsschub in der Hochschullehre einen neuen Impuls. Als Reaktion darauf wurde an der Universität des Saarlandes die Initiative "KI in der Lehre" ins Leben gerufen, die Teil des seit 2021 von der Stiftung Innovation in der Hochschullehre geförderten Projekts Digital Teaching Plug-in (DaTa-Pin) bildet. Herausforderungen und Möglichkeiten der Nutzung von KI sollen in innovativen Lehr- und Lernangeboten erprobt und ausgelotet, im Idealfall in Best-Practice-Ansätze überführt werden. Die Besonderheit des von der Fachrichtung Musikwissenschaft betreuten Teilprojekts "KI und Musik" bildet sein multimodaler Ansatz. Im Gegensatz zu vielen anderen geisteswissenschaftlichen Fächern sind dort KI-Anwendungen nicht auf Schrift oder Texte beschränkt, sondern beziehen Ton und Musik mit ein. In diesem Beitrag werden das auf zwei Semester angelegte Teilprojekt und erste Ergebnisse vorgestellt.</p><p>Wir unteruchen den aktuellen Status Quo der Entwicklung von Ontologien rhetorischer Figuren. Unser Fokus liegt dabei vor allem auf englisch-, serbisch- und deutschsprachigen Modellen. Wir geben zuerst einen Überblick über die Entwicklung und Kategorisierung von rhetorischen Figuren, um die Notwendigkeit für Ontologien hervorzuheben. Auch präsentieren wir aktuelle Ontologien in Serbisch, Deutsch und Englisch, sowie eine mehrsprachige Ontologie, die eine Vereinigung der vorigen Ontologien darstellt. Wir geben darüber hinaus einen Ausblick, wie die Zukunft bezüglich automatisierter Erkennung rhetorischer Figuren im Kontext großer Sprachmodelle ("Large Language Models") aussieht.</p><p>The project DALIA - Knowledge Base for "FAIR data usage and supply" produces and supplies an infrastructure for educational resources. An important contribution to this project is the development of a platform for teaching and training materials. It serves as a technical link between NFDI consortia and addresses an interdisciplinary research community. It is based on the DALIA Knowledge Graph which serves as an interlink of the heterogeneous and subject-specific teaching and learning materials. This poster provides the compilation of the controlled vocabularies concerning the human entities: contributors, users of the front-end, and their roles in accordance with the user roles of the platform. The focus here is on administrative metadata for human entities. Our aim is to create a hierarchy of simple metadata categories with mappings to other metadata schemas and a small (closed) core set for content curation, searching and harvesters.</p><p>Grundlage der textorientierten Forschung in den DH ist die Verfügbarkeit von maschinenlesbarem Text. Diese Anforderung kann bei digitalen Texten einfacher erfüllt werden als bei historischen Texten, wo zunächst eine Transformation in eine digitale Repräsentation zu realisieren ist. Mit der Anwendung des maschinellen Lernens in der automatischen Texterkennung ist ein enormer Fortschritt vollzogen worden. Dies betrifft die Zeichenerkennung und deren Genauigkeit. Hierbei kommen Methoden zum Einsatz, die dem Paradigma Lernen aus Beispielen folgen. Die dazu nötigen Trainingsdaten werden als Ground Truth (GT) bezeichnet. Die Erstellung von GT erfolgt zu einem Großteil manuell. Das erfordert einen hohen zeitlichen und finanziellen Aufwand. Aus diesem Grund entwickelt, pflegt, vermittelt und diskutiert das Projekt OCR-D u.a. GT-Richtlinien. Diese Richtlinien werden in einer kollaborativen Datenkultur verpflichtenden Umgebung erstellt und sollen sicherstellen, dass der Aufwand der GT-Erstellung minimiert werden kann. Im Rahmen des Workshops soll am Beispiel der Forschungsdatennutzung des Deutschen Textarchivs diese Datenkultur gemeinsam gelebt werden.</p><p>Denkmalinventare existieren in Europa seit dem 19. Jahrhundert, doch bis heute sind nur wenige digital verfügbar. Der folgende Beitrag erörtert, wie relevante Daten zum Denkmalbestand der DDR und der BRD in (digitalen) Archiven/Repositorien zu finden sind und warum es relevant ist, Informationen zu Denkmalbeständen aus analogen Quellen in digitale Formate zu transformieren. Weiterhin wird die Datenqualität aktueller digitaler Daten zum Denkmalbestand in Deutschland diskutiert, um dann das weitere Vorgehen im Hinblick auf die Visualisierung verschiedener Denkmalbestände und den Einsatz einer Python-Entwicklungsumgebung und der Python-Programmbibliothek folium abzuleiten. Abschließend wird am Fallbeispiel des Berliner Denkmalbestandes zum Zeitpunkt der Zusammenführung der Denkmallisten aus der DDR und der BRD aufgezeigt, dass nicht erst seit den "tear this down"-Forderungen Bewegung in diverse Denkmalbestände geraten ist und dass diese Arten von Aushandlungsprozessen immer auch mit Herrschaftsansprüchen und Machtbestrebungen verbunden sind.</p> </html>