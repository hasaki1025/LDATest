 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Der SSH <span style="color: rgb(2, 253, 0);">Open</span> Marketplace ( ist eine Webplattform, in der Forscher:innen gezielt nach digitalen Tools und Services suchen können. Um Tools und Services besser auffindbar zu machen, können neben ausführlichen Metadaten auch Referenzen zu Publikationen, Trainingsmaterialien, Datensätzen sowie Workflows hergestellt werden. Workflows orientieren sich dabei an Forschungsfragen sowie an "digital best practices" eines Fachbereichs, die in Form einer Vorgehensweise beschrieben werden, um ein gewünschtes Ergebnis zu erreichen. Ein Workflow ist unterteilt in Schritte ("steps"), die vorgenommen werden sollten, um die jeweilige Fragestellung beantworten zu können. Sowohl der Workflow selbst als auch die jeweiligen Schritte können mit Tools und Services verknüpft werden, die zur Umsetzung verwendet werden können. Somit erhalten Tools einen Kontext, in dem sie beispielhaft verwendet werden können, was neben einer verbesserten Auffindbarkeit auch mögliche Anwendungsgebiete aufzeigt. Im Workshop werden Teilnehmer:innen eingeladen, anhand ihrer DH-Praktiken und -Methoden Workflows für den SSH <span style="color: rgb(2, 253, 0);">Open</span> Marketplace zu erstellen und zu publizieren.</p><p>Die philosophische Tradition enzyklopädischer und fragmentarisch organisierter Wissenformationen erfährt durch neuartige, genuin digitale Möglichkeiten neue Chancen und Herausforderungen. Wörterbücher und Lemmatalisten können nun in Wikis abgebildet und auf neue Weisen hyperreferenziert, publiziert, visualisiert und rezipiert werden. Ein solches semantisches Wiki ("PhiWiki") wird derzeit als Kooperationsprojekt der Akademie der Wissenschaften und der Literatur Mainz und der Arbeitsgruppe "Philosophie der Digitalität / philosophische Digitalitätsforschung" der Deutschen Gesellschaft für Philosophie unter Mitwirkung der Technischen Informationsbibliothek Hannover erarbeitet. Inhaltlich geht es um den Themenkomplex Philosophie der Digitalität und damit um die Neudeutung philosophischer Begriffe und die philosophische (Neu-)Deutung digitaltechnischer Begriffe. Neben den technischen Aspekten geht der Vortrag auch (kurz) auf Fragen der Organisation eines solchen Wikis ein (Strukturierung der Community und des Diskurses), sowie auf wissenschaftstheoretische Reflexionen. Eine Verortung der Philosophie als digitale Geisteswissenschaft im Spannungsfeld "digitale Philosophie" und "Philosophie der Digitalität" wird versucht.</p><p>Die Zusammenarbeit zwischen Forschenden und Bürger*innen in Form von Citizen Science erfreut sich wachsender Beliebtheit in den digitalen Geisteswissenschaften. Um Teilnehmende für geisteswissenschaftliche Forschungsprojekte zu gewinnen, setzen Forschende teilweise auf Citizen Science-Projektplattformen, wie Bürger schaffen Wissen, Österreich forscht oder Zooniverse. Auch spezialisierte Plattformen, die nur ein ganz bestimmtes Thema (z.B. Linguistik oder Kunstgeschichte) oder eine Methode (z.B. Transkription oder Übersetzung) abdecken, sind in der geisteswissenschaftlichen Citizen Science-Landschaft zu finden. Daher wird in diesem Beitrag untersucht, inwieweit diese Plattformen die Teilnehmenden (sowie die Forschenden) dabei unterstützen, eine Gemeinschaft zu bilden und inwieweit diese Aufgabe den Projekten selbst überlassen wird. Erste Zwischenergebnisse machen deutlich, dass Community Building von Teilnehmenden in Projekten, die auf Citizen Science-Projektplattformen geführt werden, in erster Linie auf Projektebene selbst (und nur teilweise über diese Plattformen) stattfindet. Daher bleibt offen, ob ein Community Building von Teilnehmenden in Citizen Science-Projekten auch über die Projektgrenzen hinaus, einen Mehrwert bieten könnte.</p><p>Sogenannte ‘Kleine Fächer’, wie auch auf Regionen des Globalen Südens bzw. andere Länder der Welt spezialisierte Forschende in allen Disziplinen, sind im Rahmen ihrer digitalen Forschung immer wieder mit Herausforderungen bei Ressourcen, Tools und Infrastrukturen konfrontiert. Diese beziehen sich z.B. auf die Verfügbarkeit von Daten und Metadaten in verschiedenen Sprachen, insbesondere in nichtlateinischen Schriften, die Verwendbarkeit von Tools wie auch die allgemeine Förderung zum Datenkorpora-Aufbau. Das Panel nimmt sich dieser Themen an und diskutiert anhand von User Stories der Panelist:innen aus der Arabistik, Computerlinguistik, Religionswissenschaft, Romanistik, Sinologie und Ukrainistik sowohl allgemeine Fragen zu Ungleichheiten in Infrastrukturen zu verschiedenen Schriften und Sprachen, als auch zu konkreten Beispielen die Auswirkungen dieser Ressourcenlücken in den DH. Abschließend sollen Zukunftsperspektiven für die Stärkung multilingual ausgerichteter DH in den sogenannten Kleinen Fächern und darüber hinaus formuliert werden.</p><p>In diesem Poster beschreiben wir, wie die Forschungsdatenplattform Discuss Data um einen Bereich ("Communityspace") für die Digital Humanities erweitert wird. Dazu ergründen wir die Spezifika dieses Forschungsbereichs für einen entsprechenden Communityspace in Discuss Data und hinterfragen auch kritisch, wie erfolgreich bisherige Ansätze der Plattform Discuss Data im Aufbau der Community des Spaces für die "Osteuropa, Südkaukasus und Zentralasien" Forschung verlaufen sind und wie diese Erfahrungen bei dem Aufbau eines neuen Communityspaces einbezogen werden können. Dies betrifft auch Kernbestandteile, wie die Vernetzungskomponenten und die Möglichkeit für Diskussionen über Daten auf der Plattform.</p><p>Die Digitalisierung und Verknüpfung von Kulturerbedaten eröffnet neue Möglichkeiten für die Forschung - nicht nur in den (digitalen) Geisteswissenschaften.. Das H2020-Projekt "InTaVia" (InTangible European Heritage Visual Analysis, Curation and Communication, integriert kulturelle Objektdaten und strukturierte Biografiedaten-Bestände mehrerer Länder in einem Wissensgraphen. Für eine bessere Zugänglichkeit dieser kulturellen Informationen, entwickelt InTaVia ein Frontend mit verschiedenen Datenvisualisierung (inkl. Karten, Netzwerke, und Zeitstrahlen) welche die visuelle Analyse und Kommunikation von kulturellen Informationen möglich machen. Dieser Beitrag stellt die Ergebnisse des Projektes vor und diskutiert kritisch seine Implikationen für die Forschung an und den Umgang mit digitalisierten Kulturerbedaten.</p><p>In einem Mixed-Methods-Ansatz untersuchen wir den Zusammenhang zwischen Konflikthaftigkeit und Soundscapes (sound + landscape) in Prosatexten des deutschsprachigen Realismus und Naturalismus. Lautstärkewerte werden dabei auf der Basis eines eigens erstellten Geräuschlexikons automatisch annotiert und Konfliktwerte nach einem heuristischen Verfahren auf der Basis der Word Embedding-basierten Sentimentanalyse berechnet. Aufgrund der mit dem Naturalismus häufig verbundenen Ablehnung des bürgerlichen Realismus prüfen wir u.a., ob sich dies auch in einer in den Texten höheren Geräusch- und Konfliktintensität wiederspiegelt.</p><p>Das Paper schlägt mit ATAG ausgehend von TagML eine explizite Modellierung von Text in einer Labeled-Property-Graphdatenbank vor. Das Grundmuster ist eine Kette von adressierbaren Zeichenknoten mit einer Ebene von Annotationsknoten. Alles weitere setzt sich dann aus weiteren dieser Grundmuster zusammen. Das Modell wird schon produktiv in den Publikationsumgebungen der DFG-Projekte sozinianer.de und liberepistorlarum.de eingesetzt. Aktuell wird an einem webbasierten Editor gearbeitet, mit dem die Eingabe und die Änderung von bereits vorhandenen Text ermöglicht werden soll. Damit bietet das Modell die Robustheit der in der Computerlinguistik schon lange verwendeten Standoff-Annotationsformate verbunden mit der Möglichkeit, bereits annotierten Text zu ändern.</p><p>Seit nun mehr zehn Jahren entwickeln wir in Kollaboration mit einer Vielzahl von Forschenden im Bereich der vergleichenden Sprachwissenschaft die sogenannten Cross-Linguistic Data Formats (CLDF), eine Sammlung von Standards, die -- basierend auf tabellarischen Datenformaten -- dazu dient, den großen Wissenschatz, den die linguistische Forschung in den letzten 200 Jahren erschlossen hat, so aufzubereiten, dass er systematisch aggregiert, mit anderen Datensätzen integriert, und transparent analysiert werden kann. Trotz anfänglicher Schwierigkeiten hat sich unser Bemühen als sehr erfolgreich erwiesen, auch wenn manches, von dem wir zuerst dachten, es sei leicht zu realisieren, sich als äußerst komplziert herausgestellt hat. Heute schon liegen in CLDF die größten lexikalischen und typologisch-grammatischen Sammlungen an Sprachdaten vor, und ein Ende ist bisher nicht in Sicht. In unserer Studie stellen wir vor, wie CLDF zu dem wurde, was es heute ist, und wo wir die Standardformate in der Zukunft sehen.</p> </html>