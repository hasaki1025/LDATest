 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Die AG Digital Humanities Theorie bietet diesen halbtägigen Workshop an. Der Fokus liegt auf Formaten des Theoriebezugs in der DH-Lehre. Gegenstände des Workshops sind ein von der AG erstellter Reader "DH-Theorie" sowie eine Paneldiskussion mit Expert*innen, die DH-Studiengänge im deutschsprachigen Raum konzipiert haben bzw. koordinieren. Wir fragen, wie explizit der Theoriebezug in den Studiengängen ist, welche theoretischen Grundlagen häufig herangezogen werden, welches Verständnis von Theorie zugrunde gelegt wird (Reflexion, Praxis oder Textsammlung) und wie sich wissenschaftstheoretisch die einzelnen Disziplinen in oder gegenüber den Digital Humanities positionieren. Gemeinsam wollen wir eine These auf die Frage nach dem aktuellen Stand und der weiteren Entwicklung der Rolle von Theorie in DH-Studiengängen (quo vadis?) wagen.</p><p>The workshop is planned as a tutorial opened to all participants with an interest in learning about Open Research Data practices within the Geovistory environment. The workshop is planned as a tutorial based on the community <span style="color: rgb(2, 253, 0);">project</span> "Academic Education and Careers" and is open to all participants with an interest in learning about Open Research Data practices within the Geovistory environment. The workshop has two objectives: 1) Introduce participants to the principles of FAIR and Open Research Data management, so that participants can understand the usefulness of following those standards, not only for the publication of data and its potential reuse but <span style="color: rgb(1, 254, 0);">also</span> for how it can help the researcher throughout the <span style="color: rgb(2, 253, 0);">research</span> cycle. 2) Give participants insight into a FAIR data production workflow applying different modules of the Geovistory environment (Geovistory Toolbox, Project Webpage, and SPARQL-endpoint) from data production, to publication, to analysis and reuse.</p><p>Das Interesse der DH-Community an Film- und Videoanalysen hat in den letzten Jahren stark zugenommen. Videos sind typischerweise multimodal und Analysen auf Basis manueller Annotationen nehmen viel Zeit in Anspruch. Deshalb sind Informatik-Methoden der Mustererkennung zur (semi-)automatischen Auswertung der verschiedenen Modalitäten von hoher Bedeutung, da sie Forscher:innen aus den Digital Humanities Informationen über die interne Dynamik einzelner Videos oder ganzer Korpora liefern können. Allerdings schreiten die Entwicklungen im Bereich der Künstlichen Intelligenz und dem Teilgebiet des maschinellen Lernens schnell voran und die Implementierung aktueller Methoden erfordert umfassendes technisches Verständnis und viele Rechenressourcen. Abhilfe schafft die webbasierte Plattform "TIB AV-Analytics" (TIB-AV-A), die einen niederschwelligen Zugang zur systematischen Film- und Videoanalyse anbietet. Ziel des Workshops ist, in TIB-AV-A einzuführen, Hands-on-Erfahrung zu vermitteln, Datenimport und -export vorzustellen und darüber hinaus aktiv an einer Community für TIB-AV-A zu arbeiten. Deshalb kooperieren die Entwickler:innen von TIB-AV-A auch mit der DHd AG Film & Video.</p><p>Der Workshop bietet einen praktischen Einstieg in den Bezug und die Analyse der Daten und freien digitalen Objekte der Deutschen Nationalbibliothek (DNB). Neben einem Überblick über das vielseitige Datenangebot der DNB ist das Ziel des Hands-on-Workshops, den Teilnehmer*innen eine niedrigschwellige praktische Einführung in automatisierte Abfragen über die verschiedenen offenen Bezugswege sowie geeignete Datenformate zu geben. Die Teilnehmer*innen des Workshops erhalten einen Einblick in das DNBLab als Service für Wissenschaft und Forschung sowie das Arbeiten mit dem Bestand und den Normdaten der DNB. Durch das gemeinsame Bearbeiten einer exemplarischen Fragestellung werden Grundlagen für die Entwicklung weiterer Forschungsideen geschaffen.</p><p>In den letzten 2-3 Jahren haben die vielfältigen Weisen, mit denen Bedeutung mittels großer Sprachmodelle hergestellt, untersucht und kommuniziert werden, bisherige Annahmen darüber, wie Forschende in den DH hermeneutische Bedeutung mit KI-basierten Instrumenten untersuchen, in Frage gestellt. Ziel des Panels ist es, die Konstruktion sprachlicher Bedeutung von Menschen und von großen Sprachmodellen durch eine theoretisch informierte und begrifflich differenzierte Beschreibungssprache genauer erfassen zu können, um auf diese Weise die Fortschritte, die Probleme und auch das Scheitern genauer beschreiben zu können. Geleistet wird dies durch (1) eine genauere Rekonstruktion der Bedeutungskonstruktion in großen Sprachmodellen, (2) einen systematischen Theorieimport aus der Sprachphilosophie, (3) durch einen Untersuchung des Zusammenhangs von Bedeutung, Wissen und Modell sowie (4) durch einen Blick auf die Rolle, die bedeutungsanalytische Verfahren in den Digital Humanities spielen.</p><p>Ziel der Diskussion ist es, verschiedene Fragestellungen und Herangehensweisen zu erörtern, um unterschiedliche Zielgruppen in Digital Humanities (DH) und Cultural Heritage (CH) zu erreichen und in einen Diskurs miteinzubeziehen, sowie auftretende Schwierigkeiten und Probleme in diesem Prozess aufzugreifen. Die Teilnehmer:innen des Panels setzen sich aus vier in Deutschland, Österreich, der Schweiz, den Niederlanden und Estland agierenden Vorhaben und Projekten zusammen, die sich die Aufbereitung, Bereitstellung und Benutzung von CH im Kontext der DH für verschiedene Gruppen zum Ziel gesetzt haben.</p><p>In this workshop we will use the impresso app to explore opportunities and challenges which accompany the semantic enrichment of historical newspapers. We will reflect on the added value of Natural Language Processing techniques such as topic modelling, text reuse detection and word embeddings for historians in conjunction with an introduction and critical assessment of design solutions for the scalable reading of such enriched sources. We target researchers at all (digital) skill levels.</p><p>Grundlage der textorientierten Forschung in den DH ist die Verfügbarkeit von maschinenlesbarem Text. Diese Anforderung kann bei digitalen Texten einfacher erfüllt werden als bei historischen Texten, wo zunächst eine Transformation in eine digitale Repräsentation zu realisieren ist. Mit der Anwendung des maschinellen Lernens in der automatischen Texterkennung ist ein enormer Fortschritt vollzogen worden. Dies betrifft die Zeichenerkennung und deren Genauigkeit. Hierbei kommen Methoden zum Einsatz, die dem Paradigma Lernen aus Beispielen folgen. Die dazu nötigen Trainingsdaten werden als Ground Truth (GT) bezeichnet. Die Erstellung von GT erfolgt zu einem Großteil manuell. Das erfordert einen hohen zeitlichen und finanziellen Aufwand. Aus diesem Grund entwickelt, pflegt, vermittelt und diskutiert das Projekt OCR-D u.a. GT-Richtlinien. Diese Richtlinien werden in einer kollaborativen Datenkultur verpflichtenden Umgebung erstellt und sollen sicherstellen, dass der Aufwand der GT-Erstellung minimiert werden kann. Im Rahmen des Workshops soll am Beispiel der Forschungsdatennutzung des Deutschen Textarchivs diese Datenkultur gemeinsam gelebt werden.</p><p>Im ganztägigen Workshop erlangen die Teilnehmenden erforderliche Kenntnisse, um Tools und Workflows für die Volltexterschließung unter der Vielzahl von Angeboten auszuwählen. Dabei legen wir einen besonderen Fokus auf Open-Source-Produkte wie OCR-D und OCR4all. Nach einer Einführung in OCR-Technologien folgen praktische Teile, in denen die jeweiligen Tools für je ca. 1,5 Stunden demonstriert und ausprobiert werden. Ergänzt werden diese um kurze Inputvorträge zu praktischen Fragen wie der Erstellung eines geeigneten Workflows oder zu Hinweisen, was es zu beachten gilt bei Förderanträgen für OCR-Projekte.</p> </html>