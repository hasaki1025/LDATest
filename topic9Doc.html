 <!DOCTYPE html>  
    <html lang="de">  
<head>  
    <meta charset="UTF-8">  
    <title>topic</title>  
</head>   <p>Der Workshop zielt im Sinne der FAIR-Prinzipien und im Hinblick auf konkrete Standardisierungsbestrebungen auf die Vernetzung geistes- und kulturwissenschaftlicher Daten durch Erstellung sogenannter BEACON-Dateien auf Basis der Gemeinsamen Normdatei (GND). In dem Workshop werden zunächst zwei Beispiele vorgeführt, an denen das grundlegende Vorgehen erläutert wird. Der Schwerpunkt liegt allerdings auf der praktischen Anwendung und Hands-On-Erprobung des BEACON-Verfahrens. Abgerundet wird die Veranstaltung durch Ausblicke auf erweiterte Anwendungsbereiche des Formats.</p><p>Der Workshop beschäftigt sich mit Strategien der Erschließung, Erhaltung und Dokumentation des frühen digitalen Kulturerbes. Mit dem Verfall historischer Hardware wird diese Herausforderung eminent: Zu den Ansätzen zählen daher die sogenannten Emulatoren, die die ursprünglichen historischen Computersysteme auf modernen Betriebssysstemen simulieren. Mit Hands-on-Ansatz konzipiert, ist es Ziel des Workshops, die Teilnehmenden an die Arbeit mit Emulatoren heranzuführen, um gemeinsame Grundlagen für die anschließende Diskussion des Potenzials von Retrocomputing für die Digital Humanities zu schaffen.</p><p>In den letzten 2-3 Jahren haben die vielfältigen Weisen, mit denen Bedeutung mittels großer Sprachmodelle hergestellt, untersucht und kommuniziert werden, bisherige Annahmen darüber, wie Forschende in den DH hermeneutische Bedeutung mit KI-basierten Instrumenten untersuchen, in Frage gestellt. Ziel des Panels ist es, die Konstruktion sprachlicher Bedeutung von Menschen und von großen Sprachmodellen durch eine theoretisch informierte und begrifflich differenzierte Beschreibungssprache genauer erfassen zu können, um auf diese Weise die Fortschritte, die Probleme und auch das Scheitern genauer beschreiben zu können. Geleistet wird dies durch (1) eine genauere Rekonstruktion der Bedeutungskonstruktion in großen Sprachmodellen, (2) einen systematischen Theorieimport aus der Sprachphilosophie, (3) durch einen Untersuchung des Zusammenhangs von Bedeutung, Wissen und Modell sowie (4) durch einen Blick auf die Rolle, die bedeutungsanalytische Verfahren in den Digital Humanities spielen.</p><p>Die AG Digital Humanities Theorie bietet diesen halbtägigen Workshop an. Der Fokus liegt auf Formaten des Theoriebezugs in der DH-Lehre. Gegenstände des Workshops sind ein von der AG erstellter Reader "DH-Theorie" sowie eine Paneldiskussion mit Expert*innen, die DH-Studiengänge im deutschsprachigen Raum konzipiert haben bzw. koordinieren. Wir fragen, wie explizit der Theoriebezug in den Studiengängen ist, welche theoretischen Grundlagen häufig herangezogen werden, welches Verständnis von Theorie zugrunde gelegt wird (Reflexion, Praxis oder Textsammlung) und wie sich wissenschaftstheoretisch die einzelnen Disziplinen in oder gegenüber den Digital Humanities positionieren. Gemeinsam wollen wir eine These auf die Frage nach dem aktuellen Stand und der weiteren Entwicklung der Rolle von Theorie in DH-Studiengängen (quo vadis?) wagen.</p><p>The workshop is planned as a tutorial opened to all participants with an interest in learning about Open Research Data practices within the Geovistory environment. The workshop is planned as a tutorial based on the community <span style="color: rgb(5, 250, 0);">project</span> "Academic Education and Careers" and is open to all participants with an interest in learning about Open Research Data practices within the Geovistory environment. The workshop has two objectives: 1) Introduce participants to the principles of FAIR and Open Research Data management, so that participants can understand the usefulness of following those standards, not only for the publication of data and its potential reuse but also for how it can help the researcher throughout the <span style="color: rgb(2, 253, 0);">research</span> cycle. 2) Give participants insight into a FAIR data production workflow applying different modules of the Geovistory environment (Geovistory Toolbox, Project Webpage, and SPARQL-endpoint) from data production, to publication, to analysis and reuse.</p><p>Schwerpunkt auf der Auswertung von Orts- und Personendaten. Die Teilnehmerinnen und Teilnehmer lernen, wie sie Forschungsdaten nachnutzen, in eine Graphdatenbank integrieren und dort die Beziehungen von Entitäten analysieren können sowie wie sie die Aggregation von Normdaten und weiterer Forschungsdaten für die Verfolgung wissenschaftlicher Fragestellungen nutzen können. Durch hands-on-sessions und die Vorstellung von konkreten Anwendungsbeispielen wird das Thema anschaulich und praxisnah vermittelt. Der Workshop richtet sich an Forschende mit Interesse an digitaler Geschichtswissenschaft.</p><p>Computationelle Verfahren können selten erkenntnisreich sein, wenn sie nicht mit einer manuellen, respektive qualitativen Untersuchung des Korpus einhergehen. Meist werden zu diesem Zweck Mensch-Maschine-Interaktionen umgesetzt. Doch wie verändern sich Erkenntnisse und Forschungsprozesse durch diese Erweiterungen? Noch immer ist zu wenig evaluiert, welche Konsequenzen die Anwendung von Verfahren der Digital Humanities für Forschungsergebnisse haben. Im Workshop setzen wir hier an und untersuchen gemeinsam, aufbauend auf bisherigen Studien der Workshopleitenden, wo und wie sich hermeneutische Erkenntnisproduktion verändert. Hierfür wird Topic Modeling exemplarisch herangezogen, weil dies als unüberwachtes maschinelles Lernen besonders gut geeignet ist, um komplexe und kaum erschlossene Korpora zu untersuchen. Es wird kombiniert mit manuellen Annotationen in Tradition der Grounded Theory, die mit dem Annotationstool Catma umgesetzt werden. Workshopteilnehmende lernen nicht nur das Zusammenspiel von computationellen und manuellen Arbeitsschritten kennen, sondern gestalten dies selbst und diskutieren die epistemologischen Konsequenzen. Erkenntnisse und konkretes Vorgehen sind im Anschluss auf eigene Korpora übertragbar.</p><p>Im ganztägigen Workshop erlangen die Teilnehmenden erforderliche Kenntnisse, um Tools und Workflows für die Volltexterschließung unter der Vielzahl von Angeboten auszuwählen. Dabei legen wir einen besonderen Fokus auf Open-Source-Produkte wie OCR-D und OCR4all. Nach einer Einführung in OCR-Technologien folgen praktische Teile, in denen die jeweiligen Tools für je ca. 1,5 Stunden demonstriert und ausprobiert werden. Ergänzt werden diese um kurze Inputvorträge zu praktischen Fragen wie der Erstellung eines geeigneten Workflows oder zu Hinweisen, was es zu beachten gilt bei Förderanträgen für OCR-Projekte.</p><p>Der Workshop bietet einen praktischen Einstieg in den Bezug und die Analyse der Daten und freien digitalen Objekte der Deutschen Nationalbibliothek (DNB). Neben einem Überblick über das vielseitige Datenangebot der DNB ist das Ziel des Hands-on-Workshops, den Teilnehmer*innen eine niedrigschwellige praktische Einführung in automatisierte Abfragen über die verschiedenen offenen Bezugswege sowie geeignete Datenformate zu geben. Die Teilnehmer*innen des Workshops erhalten einen Einblick in das DNBLab als Service für Wissenschaft und Forschung sowie das Arbeiten mit dem Bestand und den Normdaten der DNB. Durch das gemeinsame Bearbeiten einer exemplarischen Fragestellung werden Grundlagen für die Entwicklung weiterer Forschungsideen geschaffen.</p> </html>