{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T09:41:11.652841Z",
     "start_time": "2024-07-04T09:41:07.159425Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "\n",
    "# 这是一个示例 Python 脚本。\n",
    "\n",
    "# 按 Shift+F10 执行或将其替换为您的代码。\n",
    "# 按 双击 Shift 在所有地方搜索类、文件、工具窗口、操作和设置。\n",
    "\n",
    "import LDA\n",
    "import LanguageDetect as ld\n",
    "\n",
    "\n",
    "def show_and_save_vis(vis, filename='output.html'):\n",
    "    pyLDAvis.save_html(vis, filename)\n",
    "    pyLDAvis.show(vis)\n",
    "\n",
    "\n",
    "# 按装订区域中的绿色按钮以运行脚本。\n",
    "if __name__ == '__main__':\n",
    "    file = 'data/LDAData.xlsx'\n",
    "    data = LDA.pre_data(LDA.load_data(file))\n",
    "    datas = ld.language_detect(data)\n",
    "    vis_en = LDA.train(datas['en'], 'en')\n",
    "    vis_de = LDA.train(datas['de'], 'de')\n",
    "    show_and_save_vis(vis_de, filename='de.html')\n",
    "    show_and_save_vis(vis_en, filename='de.html')\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdnmd\\PycharmProjects\\LDAText\\LDA.py:33: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "C:\\Users\\wdnmd\\PycharmProjects\\LDAText\\LDA.py:34: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "en\n",
      "en\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "en\n",
      "de\n",
      "de\n",
      "en\n",
      "en\n",
      "en\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n",
      "de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wdnmd\\PycharmProjects\\LDAText\\LDA.py:33: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "C:\\Users\\wdnmd\\PycharmProjects\\LDAText\\LDA.py:34: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m datas \u001B[38;5;241m=\u001B[39m ld\u001B[38;5;241m.\u001B[39mlanguage_detect(data)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# LDA.train(datas['en'], 'en')\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m vis \u001B[38;5;241m=\u001B[39m LDA\u001B[38;5;241m.\u001B[39mtrain(datas[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     22\u001B[0m show_and_save_vis(vis,filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde.html\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\LDAText\\LDA.py:89\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(data, language)\u001B[0m\n\u001B[0;32m     86\u001B[0m data_words_bigrams \u001B[38;5;241m=\u001B[39m make_bigrams(data_words_nostops, bigram_mod)\n\u001B[0;32m     87\u001B[0m data_lemmatized \u001B[38;5;241m=\u001B[39m lemmatization(data_words_bigrams, nlp, allowed_postags\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNOUN\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mADJ\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVERB\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mADV\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 89\u001B[0m id2word \u001B[38;5;241m=\u001B[39m corpora\u001B[38;5;241m.\u001B[39mDictionary(data_lemmatized)\n\u001B[0;32m     90\u001B[0m texts \u001B[38;5;241m=\u001B[39m data_lemmatized\n\u001B[0;32m     91\u001B[0m corpus \u001B[38;5;241m=\u001B[39m [id2word\u001B[38;5;241m.\u001B[39mdoc2bow(text) \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LDAText\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001B[0m, in \u001B[0;36mDictionary.__init__\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_nnz \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m documents \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 78\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_documents(documents, prune_at\u001B[38;5;241m=\u001B[39mprune_at)\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_lifecycle_event(\n\u001B[0;32m     80\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     81\u001B[0m         msg\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     82\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LDAText\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001B[0m, in \u001B[0;36mDictionary.add_documents\u001B[1;34m(self, documents, prune_at)\u001B[0m\n\u001B[0;32m    201\u001B[0m         logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madding document #\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m to \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, docno, \u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    203\u001B[0m     \u001B[38;5;66;03m# update Dictionary with the document\u001B[39;00m\n\u001B[1;32m--> 204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdoc2bow(document, allow_update\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# ignore the result, here we only care about updating token ids\u001B[39;00m\n\u001B[0;32m    206\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuilt \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m from \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m documents (total \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m corpus positions)\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_docs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_pos)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\LDAText\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001B[0m, in \u001B[0;36mDictionary.doc2bow\u001B[1;34m(self, document, allow_update, return_missing)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001B[39;00m\n\u001B[0;32m    210\u001B[0m \n\u001B[0;32m    211\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    238\u001B[0m \n\u001B[0;32m    239\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(document, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m--> 241\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoc2bow expects an array of unicode tokens on input, not a single string\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    243\u001B[0m \u001B[38;5;66;03m# Construct (word, frequency) mapping.\u001B[39;00m\n\u001B[0;32m    244\u001B[0m counter \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
